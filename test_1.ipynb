{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "##%% md\n",
    " Tensor Initialization\n",
    " \n",
    " > 인용구\n",
    " \n",
    " * 목록\n",
    " + 전부\n",
    " - 동그라미로\n",
    " \n",
    " ---\n",
    " 라인추가\n",
    " \n",
    " <span style=\"color:#F86766D\">글꼴 크기 색 변경</span>\n",
    " <p style=\"font-size: 20px\">폰트사이즈</p>\n",
    " <p style=\"font-style:italic\">기울인 폰트</p>\n",
    " 굵기는 font-weight:bold 하면된다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b34309ff0d35843"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## a_tensor_initialization.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7e713e525df57ef"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T08:06:50.997889800Z",
     "start_time": "2023-09-15T08:06:50.981947Z"
    }
   },
   "id": "43f8f21bb6b68936"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PyTorch Packages\n",
    "<p> import torch </p>\n",
    "<p> import torch.nn as nn </p>\n",
    "<p> import torch.nn.functional as F </p>\n",
    "<p> import torch.optim as optim </p>\n",
    "<p> from torchvision import datasets, transforms </p>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ab94efb2a5477d2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### torch.Tensor class & torch.tensor function\n",
    "\n",
    "<p> default tensor type of torch.Tensor : float32 </p>\n",
    "<p> default tensor type of torch.tensor : int64 </p>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cbad6344ea9544d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### troch.Tensor\n",
    "아래는 torch.Tensor 클래스로 텐서를 만드는 코드이다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fb4191e634506ce"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor class\n",
    "t1 = torch.Tensor([1, 2, 3], device='cpu')\n",
    "print(t1.dtype)   # >>> torch.float32\n",
    "print(t1.device)  # >>> cpu\n",
    "print(t1.requires_grad)  # >>> False\n",
    "print(t1.size())  # torch.Size([3])\n",
    "print(t1.shape)   # torch.Size([3])\n",
    "\n",
    "# if you have gpu device\n",
    "# t1_cuda = t1.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t1_cuda = t1.cuda()\n",
    "t1_cpu = t1.cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T06:33:22.844943600Z",
     "start_time": "2023-09-15T06:33:22.816761Z"
    }
   },
   "id": "3aeb264e9fc05469"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ab6660bafc9e718b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p> 모든 tensor는 device, dtype, shape, required_grad 요소를 가지고 있다. </p>\n",
    "<p> 위 결과는 이 요소들을 모두 확인하는 코드이다. </p>\n",
    "<br>\n",
    "<p> 위 코드 아래의 주석은 텐서의 디바이스를 gpu로 바꾸는 코드다. </p>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99fe14ccf1cbf5ab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### torch.tensor\n",
    "\n",
    "아래는 torch.tensor 을 이용해 텐서를 만드는 코드이다. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbb0e986a09e19e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch.tensor function\n",
    "t2 = torch.tensor([1, 2, 3], device='cpu')\n",
    "print(t2.dtype)  # >>> torch.int64\n",
    "print(t2.device)  # >>> cpu\n",
    "print(t2.requires_grad)  # >>> False\n",
    "print(t2.size())  # torch.Size([3])\n",
    "print(t2.shape)  # torch.Size([3])\n",
    "\n",
    "# if you have gpu device\n",
    "# t2_cuda = t2.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t2_cuda = t2.cuda()\n",
    "t2_cpu = t2.cpu()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "415d7dc7b8630028"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p> dtype을 보면 torch.Tensor와 다르게 int64임을 확인할 수 있다.</p>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fa32908f6cb5409"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Size & ndims of Tensor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2104840199b64ae3"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n"
     ]
    }
   ],
   "source": [
    "a1 = torch.tensor(1)\t\t\t     # shape: torch.Size([]), ndims(=rank): 0\n",
    "print(a1.shape, a1.ndim)\n",
    "\n",
    "a2 = torch.tensor([1])\t\t  \t     # shape: torch.Size([1]), ndims(=rank): 1\n",
    "print(a2.shape, a2.ndim)\n",
    "\n",
    "a3 = torch.tensor([1, 2, 3, 4, 5])   # shape: torch.Size([5]), ndims(=rank): 1\n",
    "print(a3.shape, a3.ndim)\n",
    "\n",
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])   # shape: torch.Size([5, 1]), ndims(=rank): 2\n",
    "print(a4.shape, a4.ndim)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T08:14:08.728337400Z",
     "start_time": "2023-09-15T08:14:08.688338100Z"
    }
   },
   "id": "9fda13a7148315b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p> 1과 [1]은 다르다. 대괄호 [] 하나가 한 차원이라고 생각하면 된다. </p>\n",
    "<p> 따라서 a2과 a3 모두 1차원이다.</p>\n",
    "<br>\n",
    "<p> shape은 각 차원의 요소들의 개수이다. 가장 바깥쪽 괄호의 차원부터 적는다. </p>\n",
    "<p> a4의 shape은 가장 바깥쪽 [] 안의 요소가 5, 그 다음 [] 안의 요소는 1개 이므로 [5,1]이 된다. </p>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af866e0a61171459"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) 2\n"
     ]
    }
   ],
   "source": [
    "a5 = torch.tensor([                 # shape: torch.Size([3, 2]), ndims(=rank): 2\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T08:19:13.911185900Z",
     "start_time": "2023-09-15T08:19:13.894113700Z"
    }
   },
   "id": "2b7111ccdf0dfbfe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p> 위 텐서는 [ [], [], [], ] 으로 이루어져 있으므로 2차원이다. </p>\n",
    "<p> 가장 바깥쪽 [] 안의 요소는 3개, 그 다음 [] 안의 요소는 2개 이므로 이 텐서의 shape은 [3,2]이다. </p>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf2733bb4c42ec70"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 1]) 3\n"
     ]
    }
   ],
   "source": [
    "a6 = torch.tensor([                 # shape: torch.Size([3, 2, 1]), ndims(=rank): 3\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T08:24:49.613746100Z",
     "start_time": "2023-09-15T08:24:49.571699500Z"
    }
   },
   "id": "1508360f6a2c06d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "위 텐서는 [ [[] []], [[] []], [[] []], ] 으로 이루어져 있으므로 3차원이다.\n",
    "<br>\n",
    "가장 바깥쪽 [] 안의 요소는 3개, 그 다음 [] 안의 요소는 2개, 그 다음 [] 안의 요소는 1개 이므로 이 텐서의 shape은 [3,2,1]이다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0b18875832ff066"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a7 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 1]), ndims(=rank): 4\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eff83814f8d2cb54"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p> </p>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba89f4906a40e195"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "################################################## 1\n",
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "################################################## 2\n",
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n",
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 3 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 99\u001B[0m\n\u001B[0;32m     91\u001B[0m a10 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([                 \u001B[38;5;66;03m# shape: torch.Size([4, 1, 5]), ndims(=rank): 3\u001B[39;00m\n\u001B[0;32m     92\u001B[0m     [[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]],\n\u001B[0;32m     93\u001B[0m     [[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]],\n\u001B[0;32m     94\u001B[0m     [[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]],\n\u001B[0;32m     95\u001B[0m     [[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]],\n\u001B[0;32m     96\u001B[0m ])\n\u001B[0;32m     97\u001B[0m \u001B[38;5;28mprint\u001B[39m(a10\u001B[38;5;241m.\u001B[39mshape, a10\u001B[38;5;241m.\u001B[39mndim)\n\u001B[1;32m---> 99\u001B[0m a11 \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43m                 \u001B[49m\u001B[38;5;66;43;03m# ValueError: expected sequence of length 3 at dim 3 (got 2)\u001B[39;49;00m\n\u001B[0;32m    100\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    101\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    102\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    103\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    104\u001B[0m \u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mValueError\u001B[0m: expected sequence of length 3 at dim 3 (got 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "a7 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 1]), ndims(=rank): 4\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)\n",
    "\n",
    "a8 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3]), ndims(=rank): 4\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n",
    "\n",
    "\n",
    "a9 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3, 1]), ndims(=rank): 5\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 5]), ndims(=rank): 2\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 1, 5]), ndims(=rank): 3\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a11 = torch.tensor([                 # ValueError: expected sequence of length 3 at dim 3 (got 2)\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-14T11:14:32.270322500Z"
    }
   },
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
